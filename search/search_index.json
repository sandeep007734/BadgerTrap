{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome # Our aim is to run the BadgerTrap tool on the latest kernel version and get some information out of it. The officially supported version is 3.21.3 . This version fails to boot with the latest Qemu system. We are following the instructions mentioned here to run a custom kernel on the Qemu system. The oldest kernel that successfully runs is 3.16.35 . However, there are couple of changes in the code since the last BadgerTrap supported kernel, and a copy-paste job will not do. We need to understand the working, and the code, to successfully port it. Todo Run the 3.16.35 kernel. Untouched. Study the old kernel and see the difference. Understand the page table working completely. How can we get access to the entries within the page offset. Run the vanilla version of the BadgerTrap Run the modified version","title":"Home"},{"location":"#welcome","text":"Our aim is to run the BadgerTrap tool on the latest kernel version and get some information out of it. The officially supported version is 3.21.3 . This version fails to boot with the latest Qemu system. We are following the instructions mentioned here to run a custom kernel on the Qemu system. The oldest kernel that successfully runs is 3.16.35 . However, there are couple of changes in the code since the last BadgerTrap supported kernel, and a copy-paste job will not do. We need to understand the working, and the code, to successfully port it. Todo Run the 3.16.35 kernel. Untouched. Study the old kernel and see the difference. Understand the page table working completely. How can we get access to the entries within the page offset. Run the vanilla version of the BadgerTrap Run the modified version","title":"Welcome"},{"location":"meeting_notes/","text":"A brief summary of our meeting notes. Main updates # Updates Nebo notes # Memos Paper Putting it all together","title":"Meeting notes"},{"location":"meeting_notes/#main-updates","text":"Updates","title":"Main updates"},{"location":"meeting_notes/#nebo-notes","text":"Memos Paper Putting it all together","title":"Nebo notes"},{"location":"page_table/","text":"Page table # In this post, we understand the working of a page table in the Linux kernel. As we understand, the memory management is one of the most crucial component of the kernel management. Any change in this has to go through rigorous testing and validation. Understanding the memory architecture will help us gain an insight into the kerne workings. Requirement # Virtual to physical address mapping. Source: Wikipedia There are multiple purposes of a page table: Program isolation The application does not have to worry about corrupting another application's memory space, willingly or un-willingly. The OS will ensure that any such access will result into a segmentation fault, otherwise know as SEGSEV fault. Security By ensuring that an another processing cannot snoop into my memory, the OS guarantees that the the content of my memory is protected. Please note that the OS here is a trusted entity as it has access to memory areas of all the programs. Todo How does a program like CheatEngine works then? Increase working set size Virtual memory also allows us to execute an application that requires more memory than currently present in the system. In this case, the OS will use the swap space to move-out and move-in memory pages as and when required. This is a very neat trick and is completely transparent to the applications. This will add some performance overhead due to swapping, which is typically stored in a slower storage medium, such as HDD or more recently SSD. Page table # Address translation for a 32 bit address in an X86 system using 4 level page table. Source: AMD 64 bit Programmers Manual 2 Chapter 5 As can be seen the figure above, As can be seen in the figure above, the first 9 bits are used to offset into the page map table . The base of the table is obtained from the CR3 register. This is a new addition to page table hierarchy since the page table moved from the 3-level to 4-level structure to support more memory in the 64-bit systems. Each entry in the page map table points to a page directory pointer table, also know as page global directory or pgd. This was first level in the 3-level page table, hence, the term global . The next 9 bits are used to offset into the PGD. Each PGD points to a page directory table also known as page middle directory PMD . Each entry in tha PMD points to a page table. The next 9 bits are used to offset into the page table. The page table is the last level of in-direction. Each entry in tha page table points to a page. The next 9 bits are used to offset into the page table. From the page table, we get the base address of a page. This is the granularity at which the page table operates. The last 12 bits are used to offset into the page table at byte-level address, at which the DRAM operates. Note CR3 is a control register that is used in virtual to physical address translation. Basically, during the context switch operation, the CR3 is populated with the address so as to locate the page table of the current process. TLB # Address translation is a costly process. Following the 4-level page table to resolve a virtual address to a physical address is called page table walk . This is done by the hardware so as to optimize the speed. Even then, in the worst case a single memory address may result into 4 extra memory accesses to fetch the PageMap, PGD, PMD and PTE. In some cases, it might be the case that the page has been swapped out to the disk due to high memory pressure. In that case, the page table will have the invalid flag set for that particular page. In this case, the hardware will generate a page-fault and the control is given to the OS. The OS will bring the page into the memory (may be by an eviction), set the page table entry as valid, and give the control to the hardware. To optimize this process, the hardware maintains a small cache of the recently translated addresses in a structure called the TLB or translational lookaside buffer . It leverage the spatial and the temporal locality of an application. Translation look aside buffer working. Source: Wikipedia. Info Spatial locality : If you access a particular address, then you are likely to access the nearby addresses also. Temporal locality : If you access an address, then it is likely that you will access the same address again in the near future. Huge page # As the size of the memory is increasing, the reach of the TLB is getting smaller. This causes a large TLB miss in a modern application with large memory footprint. To solve this issue, modern architecture supports pages of different sizes. X86 supports page size of 4KB, 2MB, and 1GB. This comes from this: Page offset: 12 bits i.e. 2^{12} = 4KB 2^{12} = 4KB Page table offset 9 bits. Combine this with the previous 12 bits = 2^{21} = 2MB 2^{21} = 2MB . PMD offset 9 bits. Combine this with the previous 21 bits = 2^{30} = 1GB 2^{30} = 1GB Other architectures support pages of different sizes based on their design. Interested readers can read the Haweye paper to know more about the huge page.","title":"Page Table"},{"location":"page_table/#page-table","text":"In this post, we understand the working of a page table in the Linux kernel. As we understand, the memory management is one of the most crucial component of the kernel management. Any change in this has to go through rigorous testing and validation. Understanding the memory architecture will help us gain an insight into the kerne workings.","title":"Page table"},{"location":"page_table/#requirement","text":"Virtual to physical address mapping. Source: Wikipedia There are multiple purposes of a page table: Program isolation The application does not have to worry about corrupting another application's memory space, willingly or un-willingly. The OS will ensure that any such access will result into a segmentation fault, otherwise know as SEGSEV fault. Security By ensuring that an another processing cannot snoop into my memory, the OS guarantees that the the content of my memory is protected. Please note that the OS here is a trusted entity as it has access to memory areas of all the programs. Todo How does a program like CheatEngine works then? Increase working set size Virtual memory also allows us to execute an application that requires more memory than currently present in the system. In this case, the OS will use the swap space to move-out and move-in memory pages as and when required. This is a very neat trick and is completely transparent to the applications. This will add some performance overhead due to swapping, which is typically stored in a slower storage medium, such as HDD or more recently SSD.","title":"Requirement"},{"location":"page_table/#page-table_1","text":"Address translation for a 32 bit address in an X86 system using 4 level page table. Source: AMD 64 bit Programmers Manual 2 Chapter 5 As can be seen the figure above, As can be seen in the figure above, the first 9 bits are used to offset into the page map table . The base of the table is obtained from the CR3 register. This is a new addition to page table hierarchy since the page table moved from the 3-level to 4-level structure to support more memory in the 64-bit systems. Each entry in the page map table points to a page directory pointer table, also know as page global directory or pgd. This was first level in the 3-level page table, hence, the term global . The next 9 bits are used to offset into the PGD. Each PGD points to a page directory table also known as page middle directory PMD . Each entry in tha PMD points to a page table. The next 9 bits are used to offset into the page table. The page table is the last level of in-direction. Each entry in tha page table points to a page. The next 9 bits are used to offset into the page table. From the page table, we get the base address of a page. This is the granularity at which the page table operates. The last 12 bits are used to offset into the page table at byte-level address, at which the DRAM operates. Note CR3 is a control register that is used in virtual to physical address translation. Basically, during the context switch operation, the CR3 is populated with the address so as to locate the page table of the current process.","title":"Page table"},{"location":"page_table/#tlb","text":"Address translation is a costly process. Following the 4-level page table to resolve a virtual address to a physical address is called page table walk . This is done by the hardware so as to optimize the speed. Even then, in the worst case a single memory address may result into 4 extra memory accesses to fetch the PageMap, PGD, PMD and PTE. In some cases, it might be the case that the page has been swapped out to the disk due to high memory pressure. In that case, the page table will have the invalid flag set for that particular page. In this case, the hardware will generate a page-fault and the control is given to the OS. The OS will bring the page into the memory (may be by an eviction), set the page table entry as valid, and give the control to the hardware. To optimize this process, the hardware maintains a small cache of the recently translated addresses in a structure called the TLB or translational lookaside buffer . It leverage the spatial and the temporal locality of an application. Translation look aside buffer working. Source: Wikipedia. Info Spatial locality : If you access a particular address, then you are likely to access the nearby addresses also. Temporal locality : If you access an address, then it is likely that you will access the same address again in the near future.","title":"TLB"},{"location":"page_table/#huge-page","text":"As the size of the memory is increasing, the reach of the TLB is getting smaller. This causes a large TLB miss in a modern application with large memory footprint. To solve this issue, modern architecture supports pages of different sizes. X86 supports page size of 4KB, 2MB, and 1GB. This comes from this: Page offset: 12 bits i.e. 2^{12} = 4KB 2^{12} = 4KB Page table offset 9 bits. Combine this with the previous 12 bits = 2^{21} = 2MB 2^{21} = 2MB . PMD offset 9 bits. Combine this with the previous 21 bits = 2^{30} = 1GB 2^{30} = 1GB Other architectures support pages of different sizes based on their design. Interested readers can read the Haweye paper to know more about the huge page.","title":"Huge page"},{"location":"badgertrap/badgertrap/","text":"In this section we will see the changes made by the BadgerTrap tool. Design # The BadgerTrap tool counts the total TLB miss of an application. This is little tricky to do. As we know from the working of the page table that in-case of an TLB miss, if the page is present in the DRAM, i.e. if the page table contains a valid entry for the page the hardware will service the TLB miss and the software will not see this. This makes counting TLB miss in the software difficult, unless hardware does it for us. BadgerTrap solved this by marking the page table entry invalid after servicing a page-fault, i.e. before returning to the hardware. We will see the exact working in the next few sections. As the entry in the page table is marked invalid, every TLB-miss will be serviced by the kernel, and hence can keep an accurate track of the TLB-misses. Please note that this causes a significant performance overhead (40X) and hence, is only used to profile the applications. Note It might not be necessary to mark the page table invalid while servicing a page fault. This can be done during the periodic timer interrupt servicing. However, this might not given an accurate picture of the TLB misses. Header file # // Process name limit #define MAX_NAME_LEN 16 // Use the reserved 52nd bit #define PTE_RESERVED_MASK (_AT(pteval_t, 1) << 51) // A 2D array to check if the current executing process is profiled or not. char badger_trap_process [ CONFIG_NR_CPUS ][ MAX_NAME_LEN ]; // This checks the 2D array and returns TRUE if it is there otherwise FALSE. int is_badger_trap_process ( const char * proc_name ); // PTE updatation tools. // Set the bit inline pte_t pte_mkreserve ( pte_t pte ); // Un-set the bit inline pte_t pte_unreserve ( pte_t pte ); // Check the bit inline int is_pte_reserved ( pte_t pte ); // PMD update methods, for huge page? // Set the bit inline pmd_t pmd_mkreserve ( pmd_t pmd ); // Un-set the bit inline pmd_t pmd_unreserve ( pmd_t pmd ); // Check the bit inline int is_pmd_reserved ( pmd_t pmd ); // Initialize the 2D array void badger_trap_init ( struct mm_struct * mm ); Data structures # System calls added # Major changes to the kernel #","title":"Badger Trap"},{"location":"badgertrap/badgertrap/#design","text":"The BadgerTrap tool counts the total TLB miss of an application. This is little tricky to do. As we know from the working of the page table that in-case of an TLB miss, if the page is present in the DRAM, i.e. if the page table contains a valid entry for the page the hardware will service the TLB miss and the software will not see this. This makes counting TLB miss in the software difficult, unless hardware does it for us. BadgerTrap solved this by marking the page table entry invalid after servicing a page-fault, i.e. before returning to the hardware. We will see the exact working in the next few sections. As the entry in the page table is marked invalid, every TLB-miss will be serviced by the kernel, and hence can keep an accurate track of the TLB-misses. Please note that this causes a significant performance overhead (40X) and hence, is only used to profile the applications. Note It might not be necessary to mark the page table invalid while servicing a page fault. This can be done during the periodic timer interrupt servicing. However, this might not given an accurate picture of the TLB misses.","title":"Design"},{"location":"badgertrap/badgertrap/#header-file","text":"// Process name limit #define MAX_NAME_LEN 16 // Use the reserved 52nd bit #define PTE_RESERVED_MASK (_AT(pteval_t, 1) << 51) // A 2D array to check if the current executing process is profiled or not. char badger_trap_process [ CONFIG_NR_CPUS ][ MAX_NAME_LEN ]; // This checks the 2D array and returns TRUE if it is there otherwise FALSE. int is_badger_trap_process ( const char * proc_name ); // PTE updatation tools. // Set the bit inline pte_t pte_mkreserve ( pte_t pte ); // Un-set the bit inline pte_t pte_unreserve ( pte_t pte ); // Check the bit inline int is_pte_reserved ( pte_t pte ); // PMD update methods, for huge page? // Set the bit inline pmd_t pmd_mkreserve ( pmd_t pmd ); // Un-set the bit inline pmd_t pmd_unreserve ( pmd_t pmd ); // Check the bit inline int is_pmd_reserved ( pmd_t pmd ); // Initialize the 2D array void badger_trap_init ( struct mm_struct * mm );","title":"Header file"},{"location":"badgertrap/badgertrap/#data-structures","text":"","title":"Data structures"},{"location":"badgertrap/badgertrap/#system-calls-added","text":"","title":"System calls added"},{"location":"badgertrap/badgertrap/#major-changes-to-the-kernel","text":"","title":"Major changes to the kernel"},{"location":"badgertrap/badgertrapinit/","text":"This explains the init procedure of the BadgerTrap /* * This function walks the page table of the process being marked for badger trap * This helps in finding all the PTEs that are to be marked as reserved. This is * espicially useful to start badger trap on the fly using (2) and (3). If we do not * call this function, when starting badger trap for any process, we may miss some TLB * misses from being tracked which may not be desierable. * * Note: This function takes care of transparent hugepages and hugepages in general. */ void badger_trap_init ( struct mm_struct * mm ) { pgd_t * pgd ; pud_t * pud ; pmd_t * pmd ; pte_t * pte ; pte_t * page_table ; spinlock_t * ptl ; unsigned long address ; unsigned long i , j , k , l ; unsigned long user = 0 ; unsigned long mask = _PAGE_USER | _PAGE_PRESENT ; struct vm_area_struct * vma ; pgd_t * base = mm -> pgd ; for ( i = 0 ; i < PTRS_PER_PGD ; i ++ ) { pgd = base + i ; if (( pgd_flags ( * pgd ) & mask ) != mask ) continue ; for ( j = 0 ; j < PTRS_PER_PUD ; j ++ ) { pud = ( pud_t * ) pgd_page_vaddr ( * pgd ) + j ; if (( pud_flags ( * pud ) & mask ) != mask ) continue ; address = ( i << PGDIR_SHIFT ) + ( j << PUD_SHIFT ); if ( vma && pud_huge ( * pud ) && is_vm_hugetlb_page ( vma )) { spin_lock ( & mm -> page_table_lock ); page_table = huge_pte_offset ( mm , address ); * page_table = pte_mkreserve ( * page_table ); spin_unlock ( & mm -> page_table_lock ); continue ; } for ( k = 0 ; k < PTRS_PER_PMD ; k ++ ) { pmd = ( pmd_t * ) pud_page_vaddr ( * pud ) + k ; if (( pmd_flags ( * pmd ) & mask ) != mask ) continue ; address = ( i << PGDIR_SHIFT ) + ( j << PUD_SHIFT ) + ( k << PMD_SHIFT ); vma = find_vma ( mm , address ); if ( vma && pmd_huge ( * pmd ) && ( transparent_hugepage_enabled ( vma ) || is_vm_hugetlb_page ( vma ))) { spin_lock ( & mm -> page_table_lock ); * pmd = pmd_mkreserve ( * pmd ); spin_unlock ( & mm -> page_table_lock ); continue ; } for ( l = 0 ; l < PTRS_PER_PTE ; l ++ ) { pte = ( pte_t * ) pmd_page_vaddr ( * pmd ) + l ; if (( pte_flags ( * pte ) & mask ) != mask ) continue ; address = ( i << PGDIR_SHIFT ) + ( j << PUD_SHIFT ) + ( k << PMD_SHIFT ) + ( l << PAGE_SHIFT ); vma = find_vma ( mm , address ); if ( vma ) { page_table = pte_offset_map_lock ( mm , pmd , address , & ptl ); * pte = pte_mkreserve ( * pte ); pte_unmap_unlock ( page_table , ptl ); } user ++ ; } } } } } Some definitions: is_vm_hugetlb_page #include <linux/mm.h> static inline bool is_vm_hugetlb_page ( struct vm_area_struct * vma ) { return !! ( vma -> vm_flags & VM_HUGETLB ); } transparent_hugepage_enabled bool transparent_hugepage_enabled ( struct vm_area_struct * vma ) { /* The addr is used to check if the vma size fits */ unsigned long addr = ( vma -> vm_end & HPAGE_PMD_MASK ) - HPAGE_PMD_SIZE ; if ( ! transhuge_vma_suitable ( vma , addr )) return false ; if ( vma_is_anonymous ( vma )) return __transparent_hugepage_enabled ( vma ); if ( vma_is_shmem ( vma )) return shmem_huge_enabled ( vma ); return false ; } /* * to be used on vmas which are known to support THP. * Use transparent_hugepage_enabled otherwise */ static inline bool __transparent_hugepage_enabled ( struct vm_area_struct * vma ) { if ( vma -> vm_flags & VM_NOHUGEPAGE ) return false ; if ( is_vma_temporary_stack ( vma )) return false ; if ( test_bit ( MMF_DISABLE_THP , & vma -> vm_mm -> flags )) return false ; if ( transparent_hugepage_flags & ( 1 << TRANSPARENT_HUGEPAGE_FLAG )) return true ; /* * For dax vmas, try to always use hugepage mappings. If the kernel does * not support hugepages, fsdax mappings will fallback to PAGE_SIZE * mappings, and device-dax namespaces, that try to guarantee a given * mapping size, will fail to enable */ if ( vma_is_dax ( vma )) return true ; if ( transparent_hugepage_flags & ( 1 << TRANSPARENT_HUGEPAGE_REQ_MADV_FLAG )) return !! ( vma -> vm_flags & VM_HUGEPAGE ); return false ; } pud_huge and pmd_huge /* * pmd_huge() returns 1 if @pmd is hugetlb related entry, that is normal * hugetlb entry or non-present (migration or hwpoisoned) hugetlb entry. * Otherwise, returns 0. */ int pmd_huge ( pmd_t pmd ) { return ! pmd_none ( pmd ) && ( pmd_val ( pmd ) & ( _PAGE_PRESENT | _PAGE_PSE )) != _PAGE_PRESENT ; } int pud_huge ( pud_t pud ) { return !! ( pud_val ( pud ) & _PAGE_PSE ); } // in an pgtable_types.h #define _PAGE_PSE (_AT(pteval_t, 1) << _PAGE_BIT_PSE)","title":"Badger Init"},{"location":"badgertrap/kernel/","text":"Kernel #","title":"Building Kernel"},{"location":"badgertrap/kernel/#kernel","text":"","title":"Kernel"}]}